---
title: "MS Exam"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning = FALSE}
library("tidyverse")
library("broom")
library("readxl")
library("ggplot2")
library("psych")
library("MASS")
```

```{r warning = FALSE}
# path setup
current_note_path <- getwd()
main_path <- file.path(current_note_path, "statistical-analysis/582/Applied_MS_Exam/")
df <- read_csv(file.path(main_path, "data/Prius_Prime_v2.csv"))
df2 <- df[-1, ]
head(df)
```


# Paired t-test
```{r warning = FALSE}
# We now know that the computer generated interval mpg is always overestimated.
t.test(df$interval_MPG, df$interval_MPG_hand_calculated, paired = TRUE, alternative = "greater")
```

# Model
## Linear Regression
```{r warning = FALSE}
measure_rmse <- function(y_true, y_pred) {
  return(sqrt(mean(y_true - y_pred)^2))
}

run_linear_regression <- function(train, test) {
  set.seed(49)
  
  model <- lm(interval_miles ~ overall_miles_lag + interval_gallons_lag + interval_miles_lag + Year + Month + day_of_week, data = train)
  
  # print(paste('coefficient = ', coef(model)))
  # print(paste('intercept = ', coef(model)[1]))
  print(summary(model))
  
  y_pred <- predict(model, newdata = test)
  
  y_train_pred <- predict(model, newdata = train)
  rmse_train <- measure_rmse(train$interval_miles, y_train_pred)
  rmse_test <- measure_rmse(test$interval_miles, y_pred)
  
  print(paste('The RMSE of the train that we will try to diminish is ', round(rmse_train, 4)))
  print(paste('The RMSE of the test is ', round(rmse_test, 4)))
  
  print(paste('r^2 train data: ', summary(model)$adj.r.squared))
  print(paste('r^2 test data: ', summary(lm(interval_miles ~ y_pred, data = test))$adj.r.squared))
  
  # Normality check
  residuals <- residuals(model)
  shapiro_test <- shapiro.test(residuals)
  print(paste('Shapiro-Wilk normality test p-value: ', shapiro_test$p.value))

  # Residual plot
  plot(fitted(model), residuals(model))
  
  # Q-Q plot
  qqnorm(resid(model))
  qqline(resid(model), col = "red")

  return (
    ggplot() +
    geom_point(aes(x = y_pred, y = y_pred - test$interval_miles), color = '#1f77b4') +
    geom_hline(yintercept = 0, color = '#d62728') +
    ggtitle('Residual Plot') +
    xlab('Predicted Values') +
    ylab('Residuals') +
    theme_minimal()
  )

}
```

## Data Split
```{r warning = FALSE}
# data split
train <- df2[df2$Year < 2023, ]
test <- df2[df2$Year >= 2023, ]
```

## Run Linear Regression
```{r warning = FALSE}
run_linear_regression(train, test)
```

## Data Split 2
Exclue the data in 2020 because there were pandemic and the data is not reliable.
```{r warning = FALSE}
df3 <- df2[df2$Year != 2020, ]
# data split
train <- df3[df3$Year < 2023, ]
test <- df3[df3$Year >= 2023, ]
run_linear_regression(train, test)
```

Box-Cox transformation of the response variable
> The Box-Cox transformation requires that all data be positive. If interval_miles includes zero or negative values, Box-Cox is not suitable without adjusting those values first.
```{r warning = FALSE}
# Perform Box-Cox transformation
bc <- boxcox(lm(interval_miles ~ overall_miles_lag + interval_gallons_lag + interval_miles_lag + Year + Month + day_of_week, data = train))
# Find the lambda that maximizes the log-likelihood
lambda <- bc$x[which.max(bc$y)]
# Apply the Box-Cox transformation to the interval_miles variable and overwirtes the original variable
train$interval_miles <- (train$interval_miles^lambda - 1) / lambda
test$interval_miles <- (test$interval_miles^lambda - 1) / lambda
# Run the model with the transformed variable
run_linear_regression(train, test)

```


Since the model after a Box-Cox transformation of the response variable did not improve the model, I finally decided to check the correlation between the explanatory variables and the response variable to see if there is a multicollinearity problem.
```{r warning = FALSE}
# https://tinyurl.com/2y644l4w
# Consider checking scatterplots of the explanatory variables against the response variable to see if there are apparent non-linear patterns that are not being modeled.
explanatory_vars <- c('overall_miles_lag', 'interval_gallons_lag', 'interval_miles_lag', 'Year', 'Month', 'day_of_week')

for (var in explanatory_vars) {
  p <- ggplot(train, aes(x = !!sym(var), y = interval_miles)) +
    geom_point() +
    geom_smooth(method = 'loess', formula = y ~ x) + # Explicitly state the formula
    labs(title = paste("Scatterplot of interval_miles vs", var),
         x = var,
         y = 'interval_miles') +
    theme_minimal()
  print(p)
}
```

