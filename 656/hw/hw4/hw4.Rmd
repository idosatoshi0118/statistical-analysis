---
title: "STAT 656 HW 4"
author: "Satoshi Ido (ID: 34788706)"
date: "November 19, 2023"
output: 
  pdf_document:
  code_folding: show
  header-includes:
    - "\\usepackage{listings}"
    - "\\lstset{breaklines=true,breakatwhitespace=true}"
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

```{r  warning=FALSE, message=FALSE, include=FALSE}
library("ggplot2")
library("tidyverse")
library("MASS")
library("fs")
library("moments")
library("rstan")
library("bayesplot")
library("StanHeaders")
library("knitr")
library("invgamma")
library("cowplot")
# opts_chunk$set(tidy = TRUE)
```

```{r warning=FALSE, message=FALSE, include=FALSE}
# set options to speed up the calculations
# To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
# For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores())
# Set the size of the plots
options(repr.plot.width = 12, repr.plot.height = 6)

# Options(bayesplot::theme_default())
bayesplot_theme_set(theme_default(base_size = 24, base_family = "sans"))
```

<center>
# Gibbs sampler for the probit model
</center>

<br><br>

Consider \( N \) pairs of observations \( (x_i , y_i) \), where \( x_i \in \mathbb{R}^d \) and \( y_i \in \{0, 1\} \). These are generated as follows: 
\begin{align*}
z_i &\sim \text{N}(x_i \beta, 1), & y_i &= 1(z_i > 0), & i &\in \{1,...,N\}.
\end{align*}
This is thus the probit model of Lecture 16-17. Write \( X \), \( Y \), and \( Z \) for the collection of \( x_i \), \( y_i \) and \( z_i \)'s. We will place a \( \text{N}(0,\Sigma_b) \) prior on \( \beta \in \mathbb{R}^d \), and draw samples from the posterior \( p(\beta,Z|X,Y) \). Observe that as in regression settings, we are really only interested in \( \beta \), the \( Z \)'s are only auxiliary variables to allow us to implement a Gibbs sampler. In other words, this is an example of a data-augmentation algorithm.

<br><br>

### Q1
> Derive and write down the conditional distribution \( Z|X, Y, \beta \).

To derive the conditional distribution \( Z|X, Y, \beta \), we need to consider the nature of the probit model and the fact that \( Z \) is essentially a latent variable that informs the observed binary outcomes \( Y \). Given the setup, \( z_i \sim N(x_i \beta, 1) \) and \( y_i = 1(z_i > 0) \), the \( P(z_i|x_i,y_i,\beta) \) is as follows:

\begin{align*}
    f(z_i|x_i,y_i,\beta) &\propto P(y_i|x_i,z_i,\beta) \\
    &= P(y_i|z_i)P(z_i|\beta,x_i)P(\beta|x_i) \\
    &\propto P(y_i|z_i)\phi(z_i; x_i \beta, 1) \\
    &= P(y_i|z_i)\frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z_i - x_i \beta)^2}{2}\right) \\
    &\propto P(y_i|z_i) \exp\left(-\frac{(z_i - x_i \beta)^2}{2}\right)
\end{align*}

   - If \( y_i = 1 \), it means \( z_i \) was greater than 0. Hence, the density function \( \phi(z_i; x_i \beta, 1) \) is truncated below at 0.
   - If \( y_i = 0 \), it means \( z_i \) was less than or equal to 0. Hence, the density function is truncated above at 0.

This means we need to normalize the truncated densities. We divide \( \phi(z_i; x_i \beta, 1) \) by the cumulative distribution function \( \Phi(0; x_i \beta, 1) \) for \( y_i = 1 \) and \( 1 - \Phi(0; x_i \beta, 1) \) for \( y_i = 0 \).
Therefore,
\begin{align*}
    f(z_i | x_i, y_i, \beta) =
    \begin{cases} 
        \frac{\phi(z_i; x_i \beta, 1)}{\Phi(0; x_i \beta, 1)} & = \frac{\frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z_i - x_i \beta)^2}{2}\right)}{\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{0} \exp\left(-\frac{(t - x_i \beta)^2}{2}\right) dt}, \quad \text{if } y_i = 1 \\
        \frac{\phi(z_i; x_i \beta, 1)}{1 - \Phi(0; x_i \beta, 1)} & = \frac{\frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z_i - x_i \beta)^2}{2}\right)}{1 - \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{0} \exp\left(-\frac{(t - x_i \beta)^2}{2}\right) dt}, \quad \text{if } y_i = 0 
    \end{cases}
\end{align*}

<br><br>

### Q2
> Derive and write down the conditional distribution \( P(\beta|X, Y, Z) \)

Given the prior \( \beta \sim N(0, \Sigma_b) \), the likelihood from the probit model, and the augmentation with \( Z \), we can derive the conditional distribution \( P(\beta|X, Y, Z) \).

The likelihood part of the posterior is proportional to \( \exp\left(-\frac{1}{2} \sum_{i=1}^{N} (z_i - x_i \beta)^2\right) \), and the prior part is proportional to \( \exp\left(-\frac{1}{2} \beta^T \Sigma_b^{-1} \beta\right) \). where \( \Sigma_b^{-1} \) is the inverse of the covariance matrix. Combining these, we get the posterior:

\begin{align*}
    p(\beta|X, Y, Z) 
    &\propto p(\beta)p(X, Y, Z | \beta) \\
    &\propto \exp\left(-\frac{1}{2} \sum_{i=1}^{N} (z_i - x_i \beta)^2 - \frac{1}{2} \beta^T \Sigma_b^{-1} \beta\right) \\
\end{align*}

### Q3
> Describe the overall Gibbs sampling algorithm, and how you would calculate the posterior mean and covariance of \(\beta \).

The Gibbs sampling algorithm involves iteratively sampling from the conditional distributions:

1. Initialize \( \beta^{(0)} \) and \( Z^{(0)} \).
2. For each iteration \( t \):
   - Sample \( Z^{(t)} \) from \( p(Z|X, Y, \beta^{(t-1)}) \) using the truncated normal distributions.
   - Sample \( \beta^{(t)} \) from \( p(\beta|X, Y, Z^{(t)}) \) using the derived normal distribution.

After a burn-in period, the samples of \( \beta \) can be used to approximate the posterior distribution.

We can estimate the posterior mean and covariance of \( \beta \) as follows:
- Posterior Mean of \( \beta \): This can be estimated by taking the average of the \( \beta \) samples after burn-in.
- Posterior Covariance of \( \beta \): This can be estimated by calculating the sample covariance of the \( \beta \) samples after burn-in.


### Q4.
> Write an R function to implement the VB algorithm for the probit model. This should accept as input a dataset (X, Y) and return the approximations to the posterior distributions over \( \beta \) and the z's. Recall how the algorithm proceeds: start with some arbitrary initial distribution q(\( \beta \)) over \( \beta \), use that to calculate the q(zi)'s, use those to calculate q(\( \beta \)), and repeat till these distributions stop changing.

In this section, we'll implement the Variational Bayes (VB) algorithm for the probit model, as discussed in Lectures 16-17. The function `probitVB` will be designed to work as follows:

- Input: The function will accept a dataset comprising two components, `X` and `Y`.
- Output: It will return approximations to the posterior distributions over the parameters \( \beta \) and the latent variables `z's`.

The algorithm follows these steps:

we use the approximation that \( P(\beta, z | X, y) \approx q(z)q(\beta) \)

1. Initialize with an arbitrary distribution \( q(\beta) \) over \( \beta \).
2. Iterate through the following steps until convergence:
   - Utilize \( q(\beta) \) to compute the distributions \( q(z_i) \) for each \( i \).
   - Use the \( q(z_i) \) distributions to update \( q(\beta) \).


The iteration continues until the changes in the distributions become negligibly small, indicating convergence.

```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80)}

probitVB <- function(X, y, sigma_beta, max.iter = 1000, burn.in = 0) {
  N <- nrow(X)          # number of observations
  d <- ncol(X)          # dimensionality of beta
  # sigma_beta <- diag(d) # prior covariance matrix for beta, can be adjusted
  xtx <- t(X) %*% X

  # initialize q(beta) as a normal distribution (mean=0, covariance=sigma_beta)
  mu_beta <- rep(0, d)
  sigma_beta_q <- solve(xtx + solve(sigma_beta))

  # initialize storage for history
  mu_beta_history <- matrix(NA, nrow = max.iter, ncol = d)
  E_z <- vector(mode = "numeric", length = N)
  Z <- matrix(nrow = max.iter, ncol = N)

  # main VB iteration loop
  for (iter in 1:max.iter) {
    # E-step: update q(z_i)'s based on current q(beta)
    mu_z <- X %*% mu_beta
    E_z[y == 1] <- mu_z[y == 1] + dnorm(-mu_z[y == 1]) / (1 - pnorm(-mu_z[y == 1]))
    E_z[y == 0] <- mu_z[y == 0] - dnorm(-mu_z[y == 0]) / pnorm(-mu_z[y == 0])
    Z[iter, ] <- E_z

    # M-step: update q(beta) based on current q(z_i)'s
    mu_beta <- sigma_beta_q %*% (t(X) %*% E_z) # update mean

    # store history
    mu_beta_history[iter, ] <- mu_beta
    Z[iter, ] <- E_z
  }

  new_list <- list(
    beta = mu_beta_history[burn.in:max.iter, ],
    z_history = Z[burn.in:max.iter, ],
    sigma_beta = sigma_beta_q,
    iterations = max.iter
  )

  return(new_list)
}

```

```{r echo=FALSE, tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80)}
set.seed(123)
X <- runif(100, -2, 2)
X <- cbind(1, X)
true_beta <- c(-0.6, 2.5)
y <- rbinom(100, 1, pnorm(true_beta %*% t(X)))
posterior_beta <- probitVB(X, y, sigma_beta=diag(ncol(X)), max.iter=5000)[[1]]
```

We can see the posterior mean converges quickly. Yet it is a bit short to the true value of \( \beta \).
```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80), fig.show="hold", out.width="80%"}

p1 <- ggplot() + 
  geom_point(aes(c(1:5000), posterior_beta[,1])) +
  labs(x = "Iteration", y = "Values of Beta[0] Parameters") +
  ggtitle("posterior mean") +
  geom_hline(yintercept = true_beta[1], color = "#403f3f", linetype = "dashed")

p2 <- ggplot() + 
  geom_point(aes(c(1:5000), posterior_beta[,2])) +
  labs(x = "Iteration", y = "Values of Beta[1] Parameters") +
  ggtitle("posterior mean") +
  geom_hline(yintercept = true_beta[2], color = "#403f3f", linetype = "dashed")

plot_grid(p1, p2, ncol = 2, nrow = 1)
```

<br><br>

<center>
# Effect of computer-generated reminders
</center>

<br><br>

An experiment was conducted to study the effects of computer-generated reminders on preventive care. Physicians (specifically, residents and faculty members on the staff of a general medicine clinic) were randomly assigned to either a treatment or control group. The treatment in this experiment is a reminder sent to physicians that encouraged them to inoculate their patients at risk for the flu. The question of interest here is whether the computer-generated reminder increases the likelihood that a patient will be vaccinated.
Your task is to use a probit regression model to learn about the effect of the treatment on patient vaccination, accounting for patient characteristics that may be associated with the response. The observed data is contained in flu data.txt, and consists of the following variables for each of the 2901 patients.

```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80)}
df <- read.csv("/Users/satoshiido/Documents/programming/statistical-analysis/656/hw/hw4/flu_data.txt", sep = " ")

# preprocess the data
df <- subset(df, age != ".")
df$age <- as.numeric(df$age) %>% scale()

y <- df$vaccinated
X <- df %>% 
  dplyr::select(-vaccinated) %>%
  as.matrix() %>%
  cbind(1, .)
colnames(X)[1] <- "intercept"
head(X)
```

### Q2
> Apply your function from the previous question on the data provided in flu_data.txt. Describe how you initialized the algorithm, the number of iterations that you ran it for, and your selected convergence criterion. Plot the approximation to the posterior distribution over \( \beta \) as well as a few of the zi's. Based on your posterior approximation, what do you conclude about the effect of the treatment on patient vaccination?

Due to limited prior knowledge, we use a weakly informative normal prior with mean 0 and variance 10 for \( \beta \). Initially, the full first order model was fit to the data using Variational Bayes. The convergence tolerance was set to 0.1.
Therefore, \( \mu_\beta \) and \( \mu_z \) were both initially set to vectors of zeros of appropriate dimension. The convergence criterion is the sum of change in parameter less than specified torlarence.
```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80)}
d <- ncol(X)
N <- nrow(X)
Sigma_beta <- diag(10, nrow = d)
posterior_flu <- probitVB(X, y, Sigma_beta, max.iter = 5000)
posterior_flu_beta <- posterior_flu[[1]]
predict_flu <- data.frame(
  "beta1" = posterior_flu_beta[c(5000),][1],
  "beta2" = posterior_flu_beta[c(5000),][2],
  "beta3" = posterior_flu_beta[c(5000),][3],
  "beta4" = posterior_flu_beta[c(5000),][4],
  "beta5" = posterior_flu_beta[c(5000),][5],
  "beta6" = posterior_flu_beta[c(5000),][6],
  "beta7" = posterior_flu_beta[c(5000),][7]
)
```

```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80), fig.show="hold", out.width="80%"}
ptreat <- ggplot() +
  geom_point(aes(c(1:5000), posterior_flu_beta[, 1])) +
  labs(x = "Iteration", y = "Values of Beta[0] Parameters") +
  ggtitle("posterior mean") +
  geom_hline(yintercept = 0, color = "#403f3f", linetype = "dashed")
```


### Q3
> Implement the probit model in Stan, and produce an MCMC approximation to the posterior. Treating these as the truth (recall MCMC becomes arbitrarily accurate with increasing number of samples), comment on your variational approximation deviates from the true posterior distribution.

The Stan code is as below:
```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80)}
probit_code = "
  data {
    int <lower=0> N;
    int <lower=0> K;
    matrix[K, K] pr_sd;
    matrix[N, K] x;
    int <lower=0, upper=1> y[N];
  }
  parameters {
    vector[K] beta;
  }
  transformed parameters {
    vector[N] mu = x*beta;
    vector[N] cd;
  // Number of observations
  // Number of predictors
  // Prior standard deviation
  // Design matrix
  // Response vector
  // Coefficients for predictors
    for (i in 1:N) {
      cd[i] = normal_cdf(mu[i], 0, 1);
    }
  }
  model {
    vector[K] mu0 = rep_vector(0, K);
    beta ~ multi_normal(mu0, pr_sd);
    y ~ bernoulli(cd);
  }
  generated quantities {
    int<lower=0> y_rep[N];
    y_rep = bernoulli_rng(cd);
  } "
probit <- stan_model(model_code = probit_code)
```

We draw 9000 samples from the approximated posterior using MCMC sampling with a burn-in period of 1000 iterations.
```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80), echo=FALSE}
ata = list(N = nrow(X), K = ncol(X), pr_sd = Sigma_b, x = X, y = Y)
nfit_probit <- sampling(probit, data = reg_data, iter = 10000, warmup = 1000, chains = 1)

```

### Q4
> 

```{r tidy=TRUE, tidy.opts=list(indent=2, width.cutoff=80), echo=FALSE}
logistic_code = "
data {
  int <lower=0> N; // number of observations
  int <lower=0> K; // number of predictors
  matrix[K, K] pr_sd; // prior standard deviation
  matrix[N, K] x; // design matrix
  int <lower=0, upper=1> y[N]; // response vector
}
parameters {
  vector[K] beta; // coefficients for predictors
}
transformed parameters {
  vector[N] mu = x*beta; // linear predictor
}
model {
  vector[K] mu0 = rep_vector(0, K); // prior mean
  beta ~ multi_normal(mu0, pr_sd); // prior
  y ~ bernoulli_logit(mu); // Likelihood
}
generated quantities {
  int<lower=0, upper=1> y_rep[N];
  y_rep = bernoulli_logit_rng(mu);
} "
logistic = stan_model(model_code = logistic_code)
```