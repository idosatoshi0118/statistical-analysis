---
title: "STAT 656 HW 2"
author: "Satoshi Ido (ID: 34788706)"
date: 1 October 2023
output: pdf_document
---
```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    message = FALSE)
```

```{r}
library("ggplot2")
library("dplyr")
library("tidyr")
library("MASS")
library("fs")
library("moments")
library("rstan")
library("bayesplot")
library("StanHeaders")
```

set options to speed up the calculations
```{r}
# to avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
# for execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores())
# set the size of the plots
options(repr.plot.width = 12, repr.plot.height = 6)

# options(bayesplot::theme_default())
bayesplot_theme_set(theme_default(base_size = 24, base_family = "sans"))
```

```{r}
# Create the input_dir (input directory)
current_note_path <- getwd()
INPUT_DIR <- file.path(current_note_path, "656/hw/hw2/data")

# If INPUT_DIR has not been created yet, create it
if (!dir.exists(INPUT_DIR)) {
  dir.create(INPUT_DIR)
}

# Create the output_dir (output directory)
OUTPUT_DIR <- file.path(current_note_path, "656/hw/hw2/outputs")

# If OUTPUT_DIR has not been created yet, create it
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR)
}

# Read CSV files using a function to specify the directory automatically
read_csv <- function(name, ...) {
  path <- file.path(INPUT_DIR, paste0(name, ".csv"))
  print(paste("Load:", path))
  return(read.csv(path, ...))
}
```

# Synthetic data
The file `hw2 synthetic.csv` is a dataset of count-valued measurements \(y = \{y_1 , \ldots , y_n\}\), with \(y_i \in \{0, 1, \ldots\}\). Each output \(y_i\) has an associated \(x_i = (x_{i,1}, x_{i,2}) \in \mathbb{R}^2\), and write \(x = \{x_1, \ldots, x_n\}^{\prime}\) as \(x\). We model \(y_i\) as
\[y_i | \beta \sim \text{Poisson}(e^{f(x_i,\beta)})\]
Here, the exponential is to ensure the Poisson rate is always positive, and the function \(f(x_i,\beta) = \beta_0 + \beta_1x_{i,1} + \beta_2x_{i,2} + \beta_3x_{i,1}^2 + \beta_4x_{i,2}^2 + \beta_5x_{i,1}x_{i,2}\).

1. With the provided data, perform a Bayesian analysis on the parameters of the model above to decide which terms in the expression for f(x) you think are important. \
State clearly what your prior over Î² is, and how you arrived at your conclusion, including any useful figures (especially of the posterior distribution). You can use Stan.
```{r}
df <- read.csv("/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/data/hw2_synthetic.csv")
head(df)
```

## Regular Poisson model
```{r}
glm1 <- glm(formula = y ~ x1 + x2 + I(x1^2) + I(x2^2) + x1:x2,
            family  = poisson(link = "log"),
            offset  = rep(1, nrow(df)),
            data    = df)
summary(glm1)
```

## Data Creation
```{r}
modMat <- as.data.frame(model.matrix(glm1))
modMat$offset <- 1
names(modMat) <- c("intercept", "x1", "x2", "x1^2", "x2^2", 
                   "x1*x2", "offset")

dat <- as.list(modMat)
dat$y <- df$y
dat$N <- nrow(modMat)
dat$p <- ncol(modMat) - 1
```

```{r}
preprocessing
```{r}
X <- df[, 0:2]
X[, "Offset"] <- 1
y <- df[, "y"]
```

model specification
```{r}

linreg_normal_code <- "
// Data are things you observe/condition on
data {
  // number of data items (an integer)
  int<lower=0> N;
  // Number of beta parameters (predictors)
  int<lower=0> p;
  int<lower=0> K;      // number of predictors
  real<lower=0> pr_sd; // std dev of the prior
  matrix[N, p-1] x;      // predictor matrix

  // Covariates
  int <lower=0, upper=1> intercept[N];
  int <lower=0, upper=1> x1[N];
  int <lower=0, upper=1> x2[N];
  int <lower=0, upper=1> x1^2[N];
  int <lower=0, upper=1> x2^2[N];
  int <lower=0, upper=1> x1*x2[N];

  // offset
  real offset[N];

  // count outcome (output vector)
  int<lower=0> y[N];

}
// Parameters to estimate
parameters {
  //vector[K] beta;       // coefficients for predictors
  real<lower=0> sigma;  // error scale
  real<lower=0> lambda[N]; // Poisson rate
  real beta[p];
}
// useful to avoid repeating calculations
// note that stan will return values of these variables
// for each MCMC sample
transformed parameters {
  
  real lp[N];
  real <lower=0> mu[N];
  vector[N] lambda = exp(x * beta); // Poisson rate
  
  for (i in 1:N) {
    // Linear predictor
    // lp[i] = poisson_log_lpmf(y[i] | lambda[i]);
    lp[i] <- beta[1] + beta[2] * x1[i] + beta[3] * x2[i] + beta[4] * x1[i]^2 + beta[5] * x2[i]^2 + beta[6] * x1[i] * x2[i] + offset[i];
    mu[i] = exp(lp[i]);

    // Mean (= lambda) for Poisson 
    mu[i] <- exp(lp[i]);
  }
}
// The actual Bayesian model goes here
// This is the prior and likelihood and I set normal dist as a prior for beta
model {
  beta ~ normal(0,pr_sd);  // Note: beta is k-dim
  sigma ~ inv_gamma(0.001,0.001);  // Close to a flat prior
  y ~ poisson(lambda);  // likelihood
}
// useful for posterior predictive checks
// Stan's posterior sampler will account for the estimation uncertainty
generated quantities {
  // define variables that are computed or generated during the execution of the model's sampling algorithm
  int<lower=0> y_tilde[N];
  for (i in 1:N) {
    y_tilde[i] = poisson_rng(lambda[i]);
  }
  vector[N] f = beta[1] + beta[2] * x[,1] + beta[3] * x[,2] + beta[4] * x[,1].^2 + beta[5] * x[,2].^2 + beta[6] * x[,1] .* x[,2];
  
} "

# build the model before sampling
linreg_normal <- stan_model(model_code = linreg_normal_code)
```

compile the model

Regression model: \(y = \{y_1 , \ldots , y_n\}\), with \(y_i \in \{0, 1, \ldots\}\). Each output \(y_i\) has an associated \(x_i = (x_{i,1}, x_{i,2}) \in \mathbb{R}^2\), and write \(x = \{x_1, \ldots, x_n\}^{\prime}\) as \(x\). We model \(y_i\) as
\[y_i | \beta \sim \text{Poisson}(e^{f(x_i,\beta)})\]
```{r}
reg1_data <- list(N = nrow(X), K = 6, pr_sd = 100, x = X, y = y)
# run the built model with given data (chains = 2 can  be okay for verification process, but it should be 4 or more for the actual analysis)
nfit <- sampling(
  linreg_normal,
  data = reg1_data,
  iter = 10000,
  warmup = 2000,
  chains = 2
  )
```

```{r}
# save the model
saveRDS(nfit, file = file.path(OUTPUT_DIR, "nfit.rds"))
# read the model (-> In this way, no need to run the model again)
# nfit <- readRDS(file = file.path(OUTPUT_DIR, "nfit.rds"))
```

```{r}
post_smp <- as.data.frame(nfit)
colnames(post_smp)[0:2] <- colnames(X)
mcmc_areas(post_smp[,0:2], pars = colnames(X)[0:2], prob = 0.8)
```

```{r}
```

```{r}
# trace plot
stan_trace(nfit, pars = c("beta", "sigma"))
```

```{r}
# posterior histogram plot
stan_hist(nfit, pars = c("beta", "sigma"))
```

```{r}
# density plot
stan_dens(nfit, pars = c("beta", "sigma"), separate_chains = TRUE)
```

```{r}
# auto correlation plot
stan_ac(nfit, pars = c("beta", "sigma"), separate_chains = TRUE)
```

```{r}
```

```{r}
```

```{r}
```

```{r}
```

# Applied problem

## first design selection
My first approach is to run the eperiments with 24 wells initially, followed by 48 wells in the second experiments. 
The reason for this is that I want to see if the 24 wells are enough to get the information about the mean and variance of the population so that I can set a reasonable prior for the second experiment.
I will assign pairs of concentrations for the two chemical modulators to each well in the manner as below. The main focus is to see how the effect of the one modulator changes depending on the concentration of the other modulator.
Since I have little idea of the effect of the modulators, I will assign the concentrations of the modulators somewhat randomly.
```{r}
design1 <- matrix(nrow = 24, ncol = 2)
# assign the concentration of the modulators randomly
## set a seed to reproduce the same result
set.seed(49)
modA <- rep(seq(0, 75, by = 15), each = 4)
modB <- rep(seq(0, 30, by = 10), times = 6)
design1 <- cbind(modA, modB)
output_path <- file.path(OUTPUT_DIR, "design1.csv")
# write a table and save it to csv
# write.table(design1, file = output_path, sep = ",", col.names = F, row.names = F)
```
