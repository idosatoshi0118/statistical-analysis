---
title: "STAT 656 HW 2"
author: "Satoshi Ido (ID: 34788706)"
date: 1 October 2023
output: pdf_document
---
```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    message = FALSE)
```

```{r}
library("ggplot2")
library("dplyr")
library("tidyr")
library("MASS")
library("fs")
library("moments")
library("rstan")
library("bayesplot")
library("StanHeaders")
```

set options to speed up the calculations
```{r}
# to avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
# for execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores())
# set the size of the plots
options(repr.plot.width = 12, repr.plot.height = 6)

# options(bayesplot::theme_default())
bayesplot_theme_set(theme_default(base_size = 24, base_family = "sans"))
```

```{r}
# Create the input_dir (input directory)
current_note_path <- getwd()
INPUT_DIR <- file.path(current_note_path, "656/hw/hw2/data")

# If INPUT_DIR has not been created yet, create it
if (!dir.exists(INPUT_DIR)) {
  dir.create(INPUT_DIR)
}

# Create the output_dir (output directory)
OUTPUT_DIR <- file.path(current_note_path, "656/hw/hw2/outputs")

# If OUTPUT_DIR has not been created yet, create it
if (!dir.exists(OUTPUT_DIR)) {
  dir.create(OUTPUT_DIR)
}

# Read CSV files using a function to specify the directory automatically
read_csv <- function(name, ...) {
  path <- file.path(INPUT_DIR, paste0(name, ".csv"))
  print(paste("Load:", path))
  return(read.csv(path, ...))
}
```

# Synthetic data
The file `hw2 synthetic.csv` is a dataset of count-valued measurements \(y = \{y_1 , \ldots , y_n\}\), with \(y_i \in \{0, 1, \ldots\}\). Each output \(y_i\) has an associated \(x_i = (x_{i,1}, x_{i,2}) \in \mathbb{R}^2\), and write \(x = \{x_1, \ldots, x_n\}^{\prime}\) as \(x\). We model \(y_i\) as
\[y_i | \beta \sim \text{Poisson}(e^{f(x_i,\beta)})\]
Here, the exponential is to ensure the Poisson rate is always positive, and the function \(f(x_i,\beta) = \beta_0 + \beta_1x_{i,1} + \beta_2x_{i,2} + \beta_3x_{i,1}^2 + \beta_4x_{i,2}^2 + \beta_5x_{i,1}x_{i,2}\).

```{r}
df <- read.csv("/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/data/hw2_synthetic.csv")
head(df)
```

Data Creation for Stan coding
```{r}
# create the data for poisson regression model
df$x1_sq <- df$x1^2
df$x2_sq <- df$x2^2
df$x1_x2 <- df$x1 * df$x2
df$offset <- 1
```

1. **With the provided data, perform a Bayesian analysis on the parameters of the model above to decide which terms in the expression for f(x) you think are important. State clearly what your prior over beta is, and how you arrived at your conclusion, including any useful figures (especially of the posterior distribution). You can use Stan.**

Summary of the data
```{r}
summary(df)
# sd of x1 and x2
paste0("sd of x1:", sd(df$x1), " sd of x2: ", sd(df$x2))
```

Histogram of data
```{r fig.show="hold", out.width="50%"}
plot(df)
hist(df$x1)
hist(df$x2)
hist(df$y)
```

I have used Stan to perform the Bayesian analysis.\
For prior distribution of beta, I have little prior knowledge of the data and model.\
Also, as we can see by plotting the `x1` and `x2` histogram, they seem to follow normal distribution. 
Therefore, I set the prior as normal distribution with a large standard deviation, N(0, 100).\

We assume
\(y = \{y_1 , \ldots , y_n\}\), with \(y_i \in \{0, 1, \ldots\}\). Each output \(y_i\) has an associated \(x_i = (x_{i,1}, x_{i,2}) \in \mathbb{R}^2\), and write \(x = \{x_1, \ldots, x_n\}^{\prime}\) as \(x\). \
We model \(y_i\) as
\[y_i | \beta \sim \text{Poisson}(e^{f(x_i,\beta)})\
] where the exponential is to ensure the Poisson rate is always positive

The Regression model which I used for the analysis is as below.\
Regression model: 
[
  f(x_i, \beta) = \beta_0 + \beta_1 x_{i, 1} + \beta_2 x_{i, 2} + \beta_3 x_{i, 1}^2 + \beta_4 x_{i, 2}^2 + \beta_5 x_{i, 1} x_{i, 2}
]
```{r}

poissonreg_normal_code = "
// Poisson model with normal prior for beta

// Data are things you observe/condition on
data {
  // number of data items
  int<lower=1> N;
  // Number of beta parameters (predictors)
  int<lower=1> p;
  // std dev of the prior
  real<lower=0> pr_sd;
  // matrix of predictors
  matrix[N, p] x;

  // offset
  // real offset[N];

  // count outcome (output vector)
  int<lower=0> y[N];
}

parameters {
  // Parameters to estimate
  vector[p] beta;
}

// useful to avoid repeating calculations
// note that stan will return values of these variables for each MCMC sample
transformed parameters {
  // exp of linear predictor
  vector[N] mu = exp(x * beta);
}

// The actual Bayesian model goes here
// I set normal dist as a prior for beta
model {
  // priors
  // Note: beta is p-dim
  beta ~ normal(0, pr_sd);  
  
  // likelihood
  y ~ poisson(mu);
}

// Generate quantities of interest (e.g. posterior predictions)
generated quantities {
  
  int<lower=0> y_rep[N];

  for (i in 1:N) {
    y_rep[i] = poisson_rng(mu[i]);
  }
}
"

# build the model before sampling
poissonreg_normal <- stan_model(model_code = poissonreg_normal_code)
```

```{r}
# create the data for stan simulation
X <- df[, c("x1", "x2", "x1_sq", "x2_sq", "x1_x2", "offset")]
y <- df$y
```

```{r}
# compile the model
poissonreg_data <- list(N = nrow(X), p = ncol(X), pr_sd = 100, x = X, y = y)
nfit <- rstan::sampling(
  object = poissonreg_normal,
  data = poissonreg_data,
  iter = 10000,
  warmup = 2000,
  chains = 2
)
```

```{r}
# save the model
# saveRDS(nfit, file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/nfit.rds")
# read the model (-> In this way, no need to run the model again)
nfit <- readRDS(file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/nfit.rds")
```


2. **Having decided which terms in f are important, keep only those and discard the rest, resulting in a possibly simpler model. Now perform a Bayesian analysis over the parameters of this model. Note that you are using the data twice, once to select the model and next to fit the it, but we will not worry about that. Compare the posteriors for both models.**

The posterior distribution of the parameters are as below.\
After considering the outputs of the model and fact that the parameter `x1_sq` and `x1_x2` are not significant in the model, I decided to remove these parameters from the model and run the model again.
As the result, \(x_1\) and \(x_2\) still seem to be significant in the model, while \(x_2 ^ 2\) seems not significant.\
There is a possibility that the \(x_2\) and \(x_2 ^ 2\) are correlated, and have a multicollinearity problem.\
Yet, it does not seem to be serious problem in this case, since the \(x_2\) and \(x_2 ^ 2\) are not highly correlated. Hence, I decided to keep the \(x_2\) in the model.

```{r}
# extract the posterior samples for further analysis
post_smp <- as.data.frame(nfit)[, c(1, 2, 3, 4, 5, 6)]
colnames(post_smp) <- colnames(X)
```

```{r fig.show="hold", out.width="50%"}
plot(post_smp$x1, type = "l")
# trace plot
# traceplot(nfit, pars = c("mu"), inc_warmup = TRUE)
```

```{r fig.show="hold", out.width="50%"}
y_rep <- extract(nfit, "y_rep")$y_rep
sg <- post_smp$mu

ppd_intervals(y_rep, x = y) +
  geom_abline(intercept = 0, slope = 1, color = "grey") +
  ggplot2::labs(y = "Predicted Ys", x = "Observed Ys")

ppd_intervals(t(t(y_rep) - y), x = y) +
  geom_abline(intercept = 0, slope = 0) +
  ggplot2::labs(y = "Errors in predicted Y's", x = "Observed Y's")
```

**model without `x1_sq` and `x1_x2`**

```{r}
X2 <- df[, c("x1", "x2", "x2_sq", "offset")]
y <- df$y
```
```{r}
# compile the model
poissonreg_data2 <- list(N = nrow(X2), p = ncol(X2), pr_sd = 100, x = X2, y = y)
nfit2 <- rstan::sampling(
  object = poissonreg_normal,
  data = poissonreg_data2,
  iter = 10000,
  warmup = 2000,
  chains = 2
)
```

```{r}
# save the model
saveRDS(nfit2, file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/nfit2.rds")
# read the model (-> In this way, no need to run the model again)
nfit2 <- readRDS(file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/nfit2.rds")
```

extract the posterior samples for further analysis
```{r}
post_smp2 <- as.data.frame(nfit2)[, c(1, 2, 3, 4)]
colnames(post_smp2) <- colnames(X2)
```

```{r fig.show="hold", out.width="50%"}
# plot comparison of the two models
mcmc_areas(post_smp, pars = colnames(X), prob = 0.8)
mcmc_areas(post_smp2, pars = colnames(X2), prob = 0.8)
```

3. **Perform posterior predictive checks for both models, being sure to explain what you are doing. Which model do you think fits the data better?**




# Applied problem

## first design selection
My first approach is to run the eperiments with 24 wells initially, followed by 48 wells in the second experiments. 
The reason for this is that I want to see if the 24 wells are enough to get the information about the mean and variance of the population so that I can set a reasonable prior for the second experiment.
I will assign pairs of concentrations for the two chemical modulators to each well in the manner as below. The main focus is to see how the effect of the one modulator changes depending on the concentration of the other modulator.
Since I have little idea of the effect of the modulators, I will assign the concentrations of the modulators somewhat randomly.
```{r}
design1 <- matrix(nrow = 24, ncol = 2)
# assign the concentration of the modulators randomly
## set a seed to reproduce the same result
set.seed(49)
modA <- rep(seq(0, 75, by = 15), each = 4)
modB <- rep(seq(0, 30, by = 10), times = 6)
design1 <- cbind(modA, modB)
# write a table and save it to csv
write.table(design1, file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/design1.csv", sep = ",", col.names = F, row.names = F)
```


## second design selection
Based on the result of the first experiment, I will take a Bayesian approach to run the second experiment with 48 wells.
Mainly, I will take the following five steps to create the second experiment design.
* Build Bayesian Model: Perform Bayesian inference on the first 24 runs to estimate the model parameters.
* Create Contour Plot: Use the estimated parameters to create a contour plot of the posterior predictive mean of conversion as a function of concentration settings for modulator A and modulator B.
* Optimization: Find the concentration settings that maximize the posterior predictive mean of conversion using an optimization algorithm.
* Posterior Predictive Distribution: Calculate the posterior predictive distribution of the concentration settings that yield maximum conversion.
* Posterior Predictive Distribution of Conversion: Construct the posterior predictive distribution of the conversion corresponding to the maximum conversion concentration settings.


```{r}
# resulf of the first experiment
design1_result <- read.table("/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/data/rslt1.csv")
colnames(design1_result) <- c("A", "B", "y")
design1_result
```

```{r}
summary(design1_result)
# sd of A and B
paste0("sd of A:", sd(design1_result$A), " | ", " sd of B: ", sd(design1_result$B), " | ", " sd of y: ", sd(design1_result$y))

```

```{r fig.show="hold", out.width="50%"}
plot(design1_result)
hist(design1_result$A)
hist(design1_result$B)
hist(design1_result$y)
```

```{r}
# mean of each level in modulator A
design1_result %>%
  group_by(A) %>%
    summarise_at(vars(y), list(name = mean))

# mean of each level in modulator B
design1_result %>%
  group_by(B) %>%
    summarise_at(vars(y), list(name = mean))
```

As we can see the result of the first design and its scatter plot, the modulators are correlated with each other and have some effect on the conversion. \
For instance, when the value of Modulator B are either 0 or 30, the conversion became lower, meanwhile it became higher when Modulator B are either 10, 20. \
Also, as we can see from the average of each level in modulator A, there seems to be a tipping point around 60.\
Therefore I set `Inver-Gamma` distribution (\sim) \((\alpha = 1, \beta = 1)\) as a prior for the parameters of the modulators, and use the following model to estimate the parameters of the modulators.

Upon runnning the model, I assume
\(y = \{y_1 , \ldots , y_n\}\), with \(y_i \in \mathbb{R}\). Each output \(y_i\) has an associated \(x_i = (x_{i,1}, x_{i,2}) \in \mathbb{R}^2\), and write \(x = \{x_1, \ldots, x_n\}^{\prime}\) as \(x\). \
We model \(y_i\) as
\[y_i | \beta \sim \text{Normal}(____)\
]


Stan code for the model simulation
```{r}
reg_half_cauchy_code = "
// regression model with normal prior for beta

// Data are things you observe/condition on
data {
  
  int<lower=1> N; // number of data items
  int<lower=1> p; // Number of beta parameters (predictors)
  matrix[N, p] x; // Matrix of predictors
  real<lower=0> pr_sd; // Prior standard deviation for beta
  real y[N]; // output vector
}

parameters {
  
  vector[p] beta; // Parameters to estimate
  real<lower=0> sigma; // Standard deviation
}

// useful to avoid repeating calculations
// note that stan will return values of these variables for each MCMC sample
transformed parameters {
  
  vector[N] mu = x * beta; // Linear predictor
}

// The actual Bayesian model goes here
// I set normal dist as a prior for beta
model {
  
  // priors
  beta ~ normal(0, pr_sd); // Note: beta is p-dim
  sigma ~ cauchy(0, 5); // Half-Cauchy prior for sigma with scale parameter 5 (adjust as needed)
  y ~ normal(mu, sigma); // likelihood
}

// Generate quantities of interest
generated quantities {
  real log_lik[N]; // log likelihood for each observation
  real y_rep[N];
  for (i in 1:N) {
    y_rep[i] = normal_rng(mu[i], sigma);
    log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); // compute log likelihood for each observation
  }
}
"

# build the model before sampling
reg_half_cauchy <- stan_model(model_code = reg_half_cauchy_code)
```

```{r}
# standardize the data
design1_result$A <- (design1_result$A - mean(design1_result$A)) / sd(design1_result$A)
design1_result$B <- (design1_result$B - mean(design1_result$B)) / sd(design1_result$B)

# create the data for a regression model
design1_result$A_sq <- design1_result$A^2
design1_result$B_sq <- design1_result$B^2
design1_result$A_B <- design1_result$A * design1_result$B

# create the data for stan simulation
X <- design1_result[, c("A", "B", "A_sq", "B_sq", "A_B")]
X[, "Offset"] <- 1
y <- design1_result$y
```

```{r}
# compile the model
reg1_data <- list(N = nrow(X), p = ncol(X), x = X, y = y, pr_sd = 100)
reg_fit <- rstan::sampling(
  object = reg_half_cauchy,
  data = reg1_data,
  iter = 10000,
  warmup = 1000,
  chains = 4
)
```

```{r}
# save the model
saveRDS(reg_fit, file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/reg_fit.rds")
# read the model
reg_fit <- readRDS(file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/reg_fit.rds")
```

Check the result of the model
```{r}
post_smp_reg <- as.data.frame(reg_fit)
# rename the columns
colnames(post_smp_reg)[1:6] <- colnames(X)
mcmc_areas(post_smp_reg[, 1:6], pars = colnames(X)[1:6], prob = 0.8)
```

Bayesian model selection
```{r}
# Compute LOO-CV for the first model
loo_all <- loo(reg_fit)

# Fit the second model (without A_sq predictor)
X_without_A_sq <- X[, !colnames(X) %in% "A_sq"]
reg1_data_without_A_sq <- list(N = nrow(X_without_A_sq), p = ncol(X_without_A_sq), x = X_without_A_sq, y = y, pr_sd = 100)
reg_fit_without_A_sq <- rstan::sampling(
  object = reg_half_cauchy,
  data = reg1_data_without_A_sq,
  iter = 10000,
  warmup = 1000,
  chains = 4
)
# Compute LOO-CV for the second model
loo_without_A_sq <- loo(reg_fit_without_A_sq)

# Fit the thrid model (without A_sq predictor)
X_without_B_sq <- X[, !colnames(X) %in% "B_sq"]
reg1_data_without_B_sq <- list(N = nrow(X_without_B_sq), p = ncol(X_without_B_sq), x = X_without_B_sq, y = y, pr_sd = 100)
reg_fit_without_B_sq <- rstan::sampling(
  object = reg_half_cauchy,
  data = reg1_data_without_B_sq,
  iter = 10000,
  warmup = 1000,
  chains = 4
)
# Compute LOO-CV for the thrid model
loo_without_B_sq <- loo(reg_fit_without_B_sq)

# Compare the models
loo::loo_compare(loo_all, loo_without_A_sq)
loo::loo_compare(loo_all, loo_without_B_sq)
loo::loo_compare(loo_without_A_sq, loo_without_B_sq)
```


Create a contour plot of the posterior predictive mean of conversion as a function of concentration settings
```{r fig.show="hold", out.width="50%"}
# extract the posterior samples for further analysis
# post_smp_reg <- as.data.frame(rstan::extract(reg_fit, permuted = TRUE))

# Extract the summary of the fit
fit_summary <- summary(reg_fit)$summary
# Extract the means of beta[1] to beta[5]
beta_means <- fit_summary[grep("beta", rownames(fit_summary)), "mean"]
# Print the extracted means
print(beta_means)

# Extract them individually
beta_1_mean <- fit_summary["beta[1]", "mean"]
beta_2_mean <- fit_summary["beta[2]", "mean"]
beta_3_mean <- fit_summary["beta[3]", "mean"]
beta_4_mean <- fit_summary["beta[4]", "mean"]
beta_5_mean <- fit_summary["beta[5]", "mean"]

# rename the columns
colnames(post_smp_reg)[1:5] <- colnames(X)

# plot the posterior distribution of the parameters
plot(post_smp_reg$A, type = "l")
plot(post_smp_reg$B, type = "l")
plot(post_smp_reg$A_sq, type = "l")
plot(post_smp_reg$B_sq, type = "l")
plot(post_smp_reg$A_B, type = "l")
```

Once we have the posterior distributions of the parameters, we can use them to generate the posterior predictive distribution of conversion for any given pair of concentrations.
Here, I also created a contour plot of the posterior predictive mean of conversion as a function of concentration settings to visualize the relationship between the modulators and conversion.

```{r}
# Define a grid of concentration settings
modulator_A_seq <- seq(0, 75, length.out = 100)
modulator_B_seq <- seq(0, 30, length.out = 100)

# standardize the data
modulator_A_seq <- (modulator_A_seq - mean(modulator_A_seq)) / sd(modulator_A_seq)
modulator_B_seq <- (modulator_B_seq - mean(modulator_B_seq)) / sd(modulator_B_seq)

grid <- expand.grid(modulator_A = modulator_A_seq, modulator_B = modulator_B_seq)

# Compute the posterior predictive mean of conversion for each pair of concentration settings
predictive_means <- apply(grid, 1, function(row) {
  modulator_A <- row["modulator_A"]
  modulator_B <- row["modulator_B"]

  # Compute the predictive mean using the model equation and the posterior samples
  mean_predictive <- mean(post_smp_reg$A * modulator_A +
                          post_smp_reg$B * modulator_B +
                          post_smp_reg$A_sq * modulator_A^2 +
                          post_smp_reg$B_sq * modulator_B^2 +
                          post_smp_reg$A_B * (modulator_A * modulator_B))
  return(mean_predictive)
})

# Add the computed predictive means to the grid
grid$predictive_mean <- predictive_means

# Plot the contour
ggplot(grid, aes(x = modulator_A, y = modulator_B, z = predictive_mean)) +
  geom_contour(aes(color = ..level..)) +
  scale_color_viridis_c() +
  labs(title = "Contour Plot of Posterior Predictive Mean of Conversion",
       x = "Modulator A",
       y = "Modulator B",
       color = "Predictive Mean") +
  theme_minimal()
```


Try without `A_sq`
```{r}
# create the data for stan simulation
X <- design1_result[, c("A", "B", "B_sq", "A_B")]
X[, "Offset"] <- 1
y <- design1_result$y

# compile the model
reg2_data <- list(N = nrow(X), p = ncol(X), x = X, y = y, pr_sd = 100)
reg_fit_without_A_sq <- rstan::sampling(
  object = reg_half_cauchy,
  data = reg2_data,
  iter = 10000,
  warmup = 1000,
  chains = 4
)
# save the model
saveRDS(reg_fit_without_A_sq, file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/reg_fit_without_A_sq.rds")
# read the model
reg_fit_without_A_sq <- readRDS(file = "/Users/satoshiido/Documents/statistical-analysis/656/hw/hw2/outputs/reg_fit_without_A_sq.rds")
# turn the posterior samples into a data frame
post_smp_reg <- as.data.frame(reg_fit_without_A_sq)
# rename the columns
colnames(post_smp_reg)[1:5] <- colnames(X)
mcmc_areas(post_smp_reg[, 1:5], pars = colnames(X)[1:5], prob = 0.8)

# compile the model
predictive_means_without_A_sq <- apply(grid, 1, function(row) {
  modulator_A <- row["modulator_A"]
  modulator_B <- row["modulator_B"]

  # Compute the predictive mean using the model equation and the posterior samples
  mean_predictive <- mean(post_smp_reg$A * modulator_A +
                          post_smp_reg$B * modulator_B +
                          post_smp_reg$B_sq * modulator_B^2 +
                          post_smp_reg$A_B * (modulator_A * modulator_B))
  return(mean_predictive)
})

# Add the computed predictive means to the grid
grid$predictive_mean <- predictive_means_without_A_sq

# Plot the contour
ggplot(grid, aes(x = modulator_A, y = modulator_B, z = predictive_mean)) +
  geom_contour(aes(color = ..level..)) +
  scale_color_viridis_c() +
  labs(title = "Contour Plot of Posterior Predictive Mean of Conversion",
       x = "Modulator A",
       y = "Modulator B",
       color = "Predictive Mean") +
  theme_minimal()

```
